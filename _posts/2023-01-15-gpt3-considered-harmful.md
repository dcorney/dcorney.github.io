---
layout: post
category: thoughts
title: GPT3 considered harmful
---

There's been increasing chatter about the potential harms that AI language generating models, most famously GPT3 and ChatGPT, might lead to. 

This includes their [potential misuse in disinformation campaigns](https://openai.com/blog/forecasting-misuse/)
and in automating [phishing scams](https://www.csoonline.com/article/3685368/study-shows-attackers-can-use-chatgpt-to-significantly-enhance-phishing-and-bec-scams.html).

To date, I've not seen any actual harm caused by GPT3 or similar models, though I'm sure that'll come all too soon.

A lot of these concerns would go away if a reliable detector could be built. A GPT-text detector could be used as a simple filter on social media platforms, email hosts and by search engine indexes to filter out (or at least clearly label) such content.

Attempts to [add watermarks to AI-generated text](https://www.newscientist.com/article/2350655-openai-is-developing-a-watermark-to-identify-work-from-its-gpt-text-ai/) will surely be bypassed quickly, potentially by down-stream paraphrasing to obscure such identifiers.

I believe detecting the output of models like GPT3 is, and will always remain, hard: the output doesn't look "machine generated" as we're used to in other fields - it's not following fixed templates or repetitive patterns. The current generation of AI art-generators may not know how many fingers the human hand usually has[^1] but AI text generators have no problem with making sure nouns and verbs agree in number, for example. To make things harder, several businesses now sell AI language generators to help people write formal business emails or web content for SEO, including people writing in their second language. And if we can't legitimately use AI tools to help us write, we'd better bin spell-checkers and grammar-checkers too. 

To me, the danger is that the content AI produces is not based on facts, but is nonetheless presented with great confidence[^2]. <i>And a lot of people also write like that.</i>  [Bullshit](https://press.princeton.edu/books/hardcover/9780691122946/on-bullshit) is [everywhere](https://www.bitebackpublishing.com/books/post-truth) and is part of being human. But it turns out you don't need to be human to bullshit. 





----
[^1]: It's four fingers and a thumb by the way - see? I'm not a robot!!
[^2]: The confidence is in the mind of the (human) reader: we tend to infer that someone is worth listening to if they're confident and don't use lots of hedging or hesitation. 
