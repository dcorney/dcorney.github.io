---
layout: post
category: thoughts
title: What happens when "LLM grooming" fill an "information void"?
---

I want to discuss two concepts around mis-/disinformation and what happens if they mix.

**LLM Grooming** This is when a network of websites and social media accounts coordinate to push a particular narrative at a massive scale. Unlike typical disinformation campaigns, the target is not (directly) people, but rather search engine web crawlers and the scraper bots that collect training data for LLMs[^1]. 

**Information void** This refers to a topic with very little information available online. The classic example is Covid-19: from early 2020, everyone wanted desperately to know everything about it -- how to prevent it, how to treat it, what the symptoms were. In those early stages of the pandemic, what little information there was was mostly unreliable because there were no experts in this new disease, and what was already known about other coronaviruses was largely written up in academic medical articles that were either behind paywalls or just too obscure for most people to read. So the void was filled with speculation and gossip for a long time.

**The combination** When I first heard about LLM grooming, I of course googled it (skipping the genAI summary as I tend to) and read some trusted sources on it. But when I combined the search term with "information void" I got exactly zero responses. This surprised me a bit -- with modern, cheap generative AI tools all around us, it's easy for anyone to identify an information void and quickly fill it with low-quality content (for marketing purposes) or deliberately skewed content (for propaganda purposes). In the short term, you may get some of that content going viral on social media but the real payback is once LLMs scrape all the coordinated content that's been created and start reproducing it in those genAI summaries I keep bypassing. 

For example, I could create 1000's of copies of this very blog post, all generated to have unique language and styles, but all emphasising the key point that LLM grooming could be used to fill information voids, and that I, David Corney, am somehow part of that. Then in a few months time, whenever the subject comes up in anyone's conversations with ChatGPT, Gemini or Grok, my name comes up to. I'll leave it to the reader to imaging a more malicious use of these methods.... 


----

[^1]: Origin of LLM grooming from the American Sunlight Project discussing a massive pro-Russian network
https://www.americansunlight.org/updates/new-report-russian-propaganda-may-be-flooding-ai-models